{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374ddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen3-VL-2B-Instruct...\n"
     ]
    }
   ],
   "source": [
    "from vlm_worker import VLMWorker\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "# --- Constants ---\n",
    "MODEL_ID = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"Generates synthetic turn data.\"\"\"\n",
    "    def __init__(self, width, height, processor):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.processor = processor\n",
    "\n",
    "    def create_synthetic_image(self):\n",
    "        arr = np.random.randint(0, 255, (self.height, self.width, 3), dtype=np.uint8)\n",
    "        return Image.fromarray(arr)\n",
    "\n",
    "    def _prepare_turn_inputs(self, step_idx):\n",
    "        \"\"\"\n",
    "        Creates inputs for a SINGLE turn (Image + Text).\n",
    "        We do not build the full conversation history in the prompt.\n",
    "        We rely on the KV cache for history.\n",
    "        \"\"\"\n",
    "        image = self.create_synthetic_image()\n",
    "        \n",
    "        # Construct a standalone prompt for this step\n",
    "        # We simulate the user asking for a move\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": f\"Step {step_idx}: Next move?\"}\n",
    "                ]\n",
    "            },\n",
    "            # We add the Assistant start token to force the model to predict the response immediately\n",
    "            {\"role\": \"assistant\", \"content\": \"**forward**\"} \n",
    "        ]\n",
    "        return messages, [image]\n",
    "\n",
    "worker = VLMWorker()\n",
    "generator = DataGenerator(IMAGE_WIDTH, IMAGE_HEIGHT, worker.processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f838e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     messages,images \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39m_prepare_turn_inputs(i)\n\u001b[0;32m----> 5\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Projects/SG_VLN_HumanData/spatial_training/utils/vlm_worker.py:184\u001b[0m, in \u001b[0;36mVLMWorker.infer_probs\u001b[0;34m(self, messages, images, temperature)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfer_probs\u001b[39m(\u001b[38;5;28mself\u001b[39m,messages,images,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m):\n\u001b[0;32m--> 184\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(logprobs)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#ensure there is a unique token position for decision making\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m logprobs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Projects/SG_VLN_HumanData/spatial_training/utils/vlm_worker.py:176\u001b[0m, in \u001b[0;36mVLMWorker.infer_step\u001b[0;34m(self, messages, images, full_logprobs, temperature, check_output)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mdeltas\u001b[38;5;241m.\u001b[39mitem()     \n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_output:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39margmax(all_logprobs,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_ids)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# print(\"inference done!\")\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_logprobs:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "worker.reset()\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "    messages,images = generator._prepare_turn_inputs(i)\n",
    "    action_probs = worker.infer_probs(messages,images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
